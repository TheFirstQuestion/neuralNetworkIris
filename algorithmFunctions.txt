Neuron:
    number outgoingValue (the outgoing signal value)
    numbers outgoingWeights (the outgoing weight values, for each neuron in the next layer)
    numbers outgoingDeltas (the change in outgoing weight values, for each neuron in the next layer)


Network:
    neuron array inputLayer (for the Iris dataset, we want 4 input neurons because we have four features we are analyzing: sepal length, sepal width, petal length, and petal width)
    neuron array hiddenLayer (the number of hidden neurons shouldn't really matter, but can affect our accuracy)
    neuron array outputLayer (we want 3 output neurons because we have three different possibilities for output in this dataset: setsota, versicolor, or virginica.)





main:
    loadData()
    splitData()
    createNetwork()
    train()
    test(trainingData)
    test(testingData)


**************************************************************** Main Functions
train():
    getIndices()
    for each epoch:
        shuffleIndices()
        calculateGlobalError()
        if global error is less than our maximum error, break (to avoid overfitting?)
        for each index:
            feedForward()
            backPropogation()


test():
    for each piece of data:
        feedForward()
        isCorrect()
        if the output layer is correct, increment count
    return number correct / number of data


feedForward():
    set outgoing values of input layer to the input data (except for bias neuron)
    for each neuron in the hidden layer:
        for each neuron in the input layer:
            sum input layer outgoing value * input layer's outgoing weight for this hidden neuron
        activationFunction(sum)
        hidden layer outgoing value = activated sum
    for each neuron in output layer:
        for each neuron in hidden layer:
            sum hidden layer outgoing value * hidden layer outgoing weight for this hidden neuron
        activationFunction(sum)
        hidden layer outgoing value = activated sum


backPropogation():
    calculate the error signal for each neuron in the output layer:
        target data value - (output layer outgoing value * activationFunctionDerivative(output layer's outgoing value))
        for each neuron in the hidden layer:
            weight change = (learning rate * error signal for this output neuron * outgoing value for this hidden neuron) + (momentum * outgoing change for this hidden neuron)
            add weight change to this hidden neuron's outgoing weight
            set this hidden neuron's outgoing weight change to weight change
    calculate the error signal for each neuron in the hidden layer:
        for each neuron in the output layer:
            add hidden neuron's outgoing weight for this output neuron * this output neuron's error
        hidden neuron's error = sum * activationFunctionDerivative(hidden layer's outgoing value)
        for each neuron in the input layer:
            weight change = (learning rate * error signal for this hidden neuron * outgoing value for this input neuron) + (momentum * outgoing change for this input neuron)
            add weight change to this input neuron's outgoing weight
            set this input neuron's outgoing weight change to weight change



************************************************************** Helper Functions
getIndices():
    get array of numbers 0 through length of data

shuffleIndices():
    for each element in the array:
        pick a random number between 0 and this element index
        swap the value at this element index with the value at that element index

activationFunction():
    sigmoid function = 1 / ( 1 + e^-x)

activationFunctionDerivative():
    derivative of sigmoid = sigmoid * (1 - sigmoid)

isCorrect():
    identify the neuron outputting the highest value:
        loop through output layer:
            if this outgoing value > current max
                this is the current max
                keep track of index
    if the value of our target data at the output layer's max value's index is 1, return true
    (highest output value = most confident)
    (target data has a 1 in the location corresponding to the correct answer)

calculateGlobalError():
    for each piece of data:
        feedForward()
        for each neuron in the output layer:
            add to sum: (target - output)^2
    divide sum by number of output neurons * length of data

createNetwork():
    create arrays of size of the number of neurons we have (+1 for the bias neuron?)
    iterate through input layer:
        create array for outgoing weights
        set each weight to a random number between 0.001 and 0.01
        create array for outgoing weight changes
        set bias neuron's value to 1, and that of the rest to 0
    iterate through hidden layer:
        create array for outgoing weights
        set each weight to a random number between 0.001 and 0.01
        create array for outgoing weight changes
        set bias neuron's value to 1, and that of the rest to 0
    iterate through output layer:
        set value to 0
        set all weights and weight changes to null (there are no outgoing weights)



**************************************************************** Data Functions
loadData():
    open the csv
    read each line:
        ignore 0th entry (line number)
        convert next four entries to numbers and store in temporary array
        convert target string to numbers, add to temporary array
        return temporary array

splitData():
    create arrays of length 80% of data and 20% of data
    randomly assign the data into the two arrays:
        getIndices()
        shuffleIndices()
        first 80% of elements in shuffled indices are training
        other 20% are testing
